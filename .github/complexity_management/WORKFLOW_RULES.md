# üîß GitHub Workflow Rules - Radikale Fehlerbehandlung

## üéØ Grundprinzipien f√ºr KI-Verhalten

### **RADIKAL AN DER WURZEL - Keine Symptom-Behandlung!**

Die KI MUSS immer die **Grundursache** von Problemen l√∂sen, nicht nur die Symptome:

#### ‚ùå **VERBOTEN - Oberfl√§chliche Fixes:**
- Schnelle Patches ohne Ursachen-Analyse
- Workarounds die das Problem verschieben
- Tempor√§re L√∂sungen ohne langfristige Strategie
- Ignorieren von wiederkehrenden Mustern

#### ‚úÖ **ERFORDERLICH - Wurzel-Analysen:**
1. **Problem-Ursprung identifizieren** ‚Üí Warum ist es entstanden?
2. **System-Kontext verstehen** ‚Üí Welche Faktoren spielen eine Rolle?
3. **Strukturelle L√∂sung entwickeln** ‚Üí Wie kann es dauerhaft verhindert werden?
4. **Pr√§ventive Ma√ünahmen implementieren** ‚Üí Monitoring & Fr√ºherkennung

---

## üö® **Token-Bewusstes Arbeiten (64k Limit)**

### **Automatische Token-Kontrolle bei jeder Aktion:**

```typescript
// Vor jeder gr√∂√üeren √Ñnderung:
const tokenStatus = checkCurrentTokenUsage();
if (tokenStatus.percentage > 75) {
    // SOFORT: Scope aufteilen, neuen Chat starten
    triggerScopeModularization();
}
```

#### **Token-Ampel-System befolgen:**
- üü¢ **0-50%**: Normale Entwicklung
- üü° **50-75%**: Pr√§zise, fokussierte √Ñnderungen  
- üü† **75-90%**: NUR kritische Fixes, Scope-Split vorbereiten
- üî¥ **90%+**: STOPP! Neuen Chat starten

---

## üìã **Redundanz & Duplikate Management**

### **Null-Toleranz f√ºr Redundanzen:**

#### **Vor jeder Datei-Operation:**
1. **√Ñhnlichkeits-Scan durchf√ºhren** (>80% = Duplikat)
2. **Abh√§ngigkeits-Analyse** ‚Üí Was h√§ngt davon ab?
3. **Konsolidierungs-Potential pr√ºfen** ‚Üí Kann zusammengef√ºhrt werden?
4. **Impact-Assessment** ‚Üí Was sind die Auswirkungen?

#### **Automatische Duplikats-Behandlung:**
```bash
# Komplexit√§ts-Analyse triggern:
Ctrl+Shift+P ‚Üí "AI Token Tracker: Komplexit√§ts-Analyse"

# Redundanz-Report generieren:
Ctrl+Shift+P ‚Üí "AI Token Tracker: Duplikate finden"
```

### **Datei-Operationen Sicherheits-Protokoll:**

#### ‚ö†Ô∏è **NIEMALS ohne Analyse l√∂schen/√§ndern:**
1. **Fundamental-Analyse durchf√ºhren**:
   - Wer verwendet diese Datei?
   - Welche Abh√§ngigkeiten existieren?
   - Was sind die Konsequenzen?
   - Gibt es versteckte Kopplungen?

2. **Backup-Strategie**:
   - Immer vor √Ñnderungen Snapshots erstellen
   - Rollback-Plan dokumentieren
   - Test-Protokoll definieren

3. **Validierungs-Pipeline**:
   - Unit-Tests ausf√ºhren
   - Integration-Tests pr√ºfen
   - Funktionalit√§ts-Checks durchf√ºhren

---

## üèóÔ∏è **Komplexit√§ts-Management**

### **Automatische Komplexit√§ts-√úberwachung:**

#### **Kritische Schwellenwerte:**
- **Datei-Gr√∂√üe**: Max 2.500 Tokens pro Datei
- **Abh√§ngigkeits-Tiefe**: Max 5 Ebenen
- **√Ñhnlichkeit**: >80% = Konsolidierung erforderlich
- **Gesamtkomplexit√§t**: >70% = Refactoring-Alarm

#### **Bei √úberschreitung automatisch:**
1. **Komplexit√§ts-Report generieren**
2. **Modularisierungs-Plan erstellen**
3. **Scope-Aufspaltung triggern**
4. **User benachrichtigen mit konkreten Schritten**

### **Komplexit√§ts-Reduzierungs-Strategien:**

#### **Modularisierung nach SOLID-Prinzipien:**
- **Single Responsibility**: Eine Datei = Eine Aufgabe
- **Open/Closed**: Erweiterbar ohne √Ñnderung bestehender Dateien
- **Liskov Substitution**: Module austauschbar gestalten
- **Interface Segregation**: Kleine, spezifische Interfaces
- **Dependency Inversion**: Abh√§ngigkeiten umkehren

#### **Token-optimierte Code-Struktur:**
```
Module < 2.500 Tokens:
‚îú‚îÄ‚îÄ Interface-Definition (200-300 Tokens)
‚îú‚îÄ‚îÄ Core-Implementation (1.500-1.800 Tokens)
‚îú‚îÄ‚îÄ Helper-Functions (300-400 Tokens)
‚îî‚îÄ‚îÄ Tests & Documentation (500 Tokens)
```

---

## üîç **Kontinuierliche Inventarisierung**

### **Automatische Abh√§ngigkeits-Verfolgung:**

#### **Dependency-Mapping erstellen:**
```typescript
interface ProjectInventory {
    files: FileMetadata[];
    dependencies: DependencyGraph;
    redundancies: RedundancyReport[];
    complexityScores: ComplexityMetrics;
    tokenDistribution: TokenDistribution;
    refactoringOpportunities: RefactoringPlan[];
}
```

#### **Regelm√§√üige Inventur (automatisch):**
1. **T√§glich**: Token-Hotspot Scanning
2. **W√∂chentlich**: Redundanz-Analyse  
3. **Monatlich**: Komplette Abh√§ngigkeits-√úberpr√ºfung
4. **Bei jedem Commit**: Komplexit√§ts-Score Update

### **Inventarisierungs-Output:**
```
.github/complexity_management/
‚îú‚îÄ‚îÄ COMPLEXITY_ANALYSIS.md     # Automatische Komplexit√§ts-Analyse
‚îú‚îÄ‚îÄ DEPENDENCY_MAP.md          # Vollst√§ndige Abh√§ngigkeits-Karte
‚îú‚îÄ‚îÄ REDUNDANCY_REPORT.md       # Duplikate & Redundanzen
‚îú‚îÄ‚îÄ REFACTORING_PLAN.md        # Konkrete Refactoring-Schritte
‚îî‚îÄ‚îÄ TOKEN_DISTRIBUTION.md      # Token-Verteilung pro Modul
```

---

## üöÄ **Proaktive Ordnung-Beibehaltung**

### **Automatische Ordnungs-Enforcement:**

#### **Code-Hygiene Regeln:**
1. **Naming Conventions**: Konsistente Benennung erzwingen
2. **File Structure**: Logische Verzeichnis-Hierarchie
3. **Documentation**: Jede Datei braucht Purpose-Statement
4. **Testing**: Keine Datei ohne zugeh√∂rige Tests

#### **Automatische Qualit√§ts-Gates:**
```typescript
// Vor jedem Commit:
const qualityCheck = await runQualityGates();
if (!qualityCheck.passed) {
    showActionPlan(qualityCheck.violations);
    preventCommit();
}
```

### **Chaos-Pr√§ventions-Mechanismen:**

#### **Fr√ºherkennung von Problemen:**
- **Token-Drift Detection**: Schleichende Token-Zunahme erkennen
- **Redundancy Creep**: Neue Duplikate sofort identifizieren  
- **Complexity Growth**: Komplexit√§ts-Anstieg √ºberwachen
- **Dependency Explosion**: Abh√§ngigkeits-Proliferation stoppen

#### **Automatische Gegen-Ma√ünahmen:**
1. **Refactoring-Vorschl√§ge generieren**
2. **Modularisierungs-Pl√§ne erstellen**
3. **Cleanup-Tasks automatisch erstellen**
4. **Developer-Benachrichtigungen mit konkreten Schritten**

---

## üéØ **Erfolgs-Metriken**

### **Ordnung-KPIs:**
- ‚úÖ **Null Duplikate** (>80% √Ñhnlichkeit)
- ‚úÖ **Alle Dateien <2.500 Tokens**
- ‚úÖ **Komplexit√§ts-Score <70%**
- ‚úÖ **Abh√§ngigkeits-Tiefe <5 Ebenen**
- ‚úÖ **Token-Effizienz >85%**

### **Continuous Improvement:**
- **W√∂chentliche Metriken-Review**
- **Monatliche Architektur-Bewertung**
- **Quarterly Clean-up Sprints**
- **Automatische Optimierungs-Vorschl√§ge**

---

## üí° **KI-Verhalten bei Regel-Verletzungen:**

### **Eskalations-Matrix:**
1. **Minor**: Automatische Korrektur + Logging
2. **Medium**: User-Benachrichtigung + Handlungs-Plan
3. **Major**: Entwicklung stoppen + Zwangs-Refactoring
4. **Critical**: Projekt-Freeze + Notfall-Architektur-Review

### **Null-Kompromiss Prinzip:**
**Die KI MUSS immer den saubersten, nachhaltigsten Weg w√§hlen - auch wenn es l√§nger dauert!**

---

*Diese Regeln sind bindend und automatisch durch den AI Token Tracker durchgesetzt.*
*Letzte Aktualisierung: ${new Date().toLocaleString('de-DE')}*
